{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chronic-cache",
   "metadata": {},
   "source": [
    "# Neural Networks Basics\n",
    "\n",
    "This notebook contains a bare-bones pipeline for training and evaluating a neural network using Pytorch.\n",
    "\n",
    "Though the network is simple and the data is generated randomly, this template contains elements that would be useful in many more complex settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-midwest",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "0. Optional but strongly recommended: install a package/environment manager like [Conda](https://docs.conda.io/projects/conda/en/latest/). I recommend [Miniconda](https://docs.conda.io/en/latest/miniconda.html) for a minimal option.\n",
    "   - Create a new environment and activate it before installing the remaining packages\n",
    "1. If using this code in a Jupyter notebook, install [Jupyter](https://jupyter.org/install) and run `jupyter notebook` from your terminal to launch a server you can use from your browser\n",
    "2. Install the current Pytorch version appropriate for your system:\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "### Other useful packages used in this code\n",
    "- numpy \n",
    "- tqdm\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seventh-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code setup\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(0)  \n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-bread",
   "metadata": {},
   "source": [
    "## Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monthly-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, splits, random=True):\n",
    "    \"\"\"split data into C partitions, equal to the length of argument splits\n",
    "    \n",
    "    X: a data matrix Nxd with N examples with d features\n",
    "    y: an Nxt matrix with targets for the examples in X. The indices should match X\n",
    "    splits: an iterable (eg list) of C proportions to split indices into C partitions\n",
    "    random (bool): sample indices randomly if True (default) or in order if False\n",
    "    \"\"\"\n",
    "    \n",
    "    # scale splits to sum to 1 in case they dont\n",
    "    splits = np.array(splits)\n",
    "    splits = splits/sum(splits) \n",
    "    \n",
    "    # Get integer number of samples in each partition based on splits\n",
    "    num_samples = X.shape[0]\n",
    "    assert y.shape[0]==num_samples, \"First dimension of data matrix X and targets y should be equal\"\n",
    "    num_splits = np.floor(num_samples * splits)\n",
    "    num_splits[-1] = num_samples - np.sum(num_splits[:-1])\n",
    "    num_splits = np.cumsum(num_splits).astype(np.int)\n",
    "    \n",
    "    # Get samples in each partition\n",
    "    if random:\n",
    "        idxs = np.random.permutation(num_samples)\n",
    "        X = X[idxs]\n",
    "        y = y[idxs]\n",
    "    X = np.split(X, num_splits[:-1])\n",
    "    y = np.split(y, num_splits[:-1])\n",
    "    \n",
    "    return zip(X, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data (replace with dataset of your choice)\n",
    "dim = 200\n",
    "num_classes = 2\n",
    "X = np.random.rand(1000, dim).astype(np.float32)\n",
    "y = np.random.choice(num_classes, 1000).reshape([-1, 1]).astype(np.float32)\n",
    "\n",
    "# 2. Split into train, validation, test\n",
    "(train_X, train_y), (val_X, val_y), (test_X, test_y) = split_data(X, y, [0.6, 0.2, 0.2])\n",
    "\n",
    "# 3. Create Pytorch datasets and dataloaders \n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_X), torch.tensor(train_y))\n",
    "val_dataset = TensorDataset(torch.tensor(val_X), torch.tensor(val_y))\n",
    "test_dataset = TensorDataset(torch.tensor(test_X), torch.tensor(test_y))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-relative",
   "metadata": {},
   "source": [
    "## Create Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "marked-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This neural network has linear (fully connected) layers, each followed ReLU activation functions.\n",
    "    The final output is passed through as sigmoid function for binary classification\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_layers=3, hidden_size=512, output_size=1):\n",
    "        super(myNN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())])\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU()))\n",
    "        self.layers.append(nn.Sequential(nn.Linear(hidden_size, output_size)))\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(model.parameters()).device\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        \"\"\"Get total number and number of trainable parameters in model\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters()), sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.sig(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-afternoon",
   "metadata": {},
   "source": [
    "## Initialize and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "worldwide-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Network \n",
    "def train_epoch(dataloader, model, loss_fxn, optimizer):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch, y_batch = x_batch.to(model.device), y_batch.to(model.device)\n",
    "        prediction = model(x_batch)\n",
    "        epoch_loss += loss_fxn(prediction, y_batch)\n",
    "\n",
    "        \n",
    "    # Backpropagation\n",
    "    epoch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "# Evaluate model\n",
    "def eval_epoch(dataloader, model, loss_fxn):\n",
    "    # loss_fxn can instead be an evaluation function, eg classification errors, to return error instead of loss\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(model.device), y_batch.to(model.device)\n",
    "            prediction = model(x_batch)\n",
    "            epoch_loss += loss_fxn(prediction, y_batch)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aging-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 366,081 total parameters and 366,081 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = myNN(input_size=dim)\n",
    "model.to(device)\n",
    "print('Model has {:,} total parameters and {:,} trainable parameters'.format(*model.num_params))\n",
    "\n",
    "# Loss function, optimizer, and training options\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 500\n",
    "loss_fxn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 317/500 [00:19<00:11, 16.60epoch/s, Train loss=0.00103, Validation loss=0.095] "
     ]
    }
   ],
   "source": [
    "pbar = tqdm.trange(1, num_epochs + 1, unit=\"epoch\")\n",
    "\n",
    "train_losses = np.zeros([num_epochs])\n",
    "val_losses = np.zeros([num_epochs])\n",
    "for epoch,_ in enumerate(pbar):\n",
    "    train_loss = train_epoch(train_loader, model, loss_fxn, optimizer)\n",
    "    val_loss = eval_epoch(val_loader, model, loss_fxn)\n",
    "    \n",
    "    train_loss = train_loss.item()/len(train_loader.dataset)\n",
    "    val_loss = val_loss.item()/len(val_loader.dataset)\n",
    "    \n",
    "    train_losses[epoch] = train_loss\n",
    "    val_losses[epoch] = val_loss\n",
    "    pbar.set_postfix({'Train loss': train_loss, 'Validation loss': val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-missile",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Before evaluating trained model on the test data, experiment with hyperparameters, including any of the following:\n",
    "- learning rate, learning rate decay schedule\n",
    "- batch size \n",
    "- number of epochs\n",
    "- number of layers\n",
    "- layer hidden dimension\n",
    "- optimizer\n",
    "- loss function\n",
    "- other architecture choices\n",
    "- regularization options (not included here, hence the poor performance on validation data)\n",
    "\n",
    "The best model will be the one that performs best on the validation data. \"Best\" is *usually* determined by the error function, not the loss function, but it depends on the learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-cocktail",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "Once the model is \"frozen,\" can evaluate on the test data set. Again, usually we want to replace the loss function with some error function instead (eg classification error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mineral-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss per test sample: 0.1031\n"
     ]
    }
   ],
   "source": [
    "test_loss = eval_epoch(test_loader, model, loss_fxn)\n",
    "print('Avg loss per test sample: {0:.4f}'.format(test_loss.item()/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-spyware",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
